<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="人工智能 geek 极客 阅读 电影"><title>PPO算法 | Light</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">PPO算法</h1><a id="logo" href="/.">Light</a><p class="description">在这座城，等一个人</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/message/"><i class="fa fa-comments"> 留言</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">PPO算法</h1><div class="post-meta">Jun 1, 2019<span> | </span><span class="category"><a href="/categories/学习笔记/">学习笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#概述"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#论文概述"><span class="toc-number">2.</span> <span class="toc-text">论文概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#相关背景"><span class="toc-number">2.1.</span> <span class="toc-text">相关背景</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#策略梯度"><span class="toc-number">2.1.1.</span> <span class="toc-text">策略梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#可信域方法"><span class="toc-number">2.1.2.</span> <span class="toc-text">可信域方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Clipped-Surrogate-Objective"><span class="toc-number">2.2.</span> <span class="toc-text">Clipped Surrogate Objective</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用KL惩罚系数"><span class="toc-number">2.3.</span> <span class="toc-text">使用KL惩罚系数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总体算法"><span class="toc-number">2.4.</span> <span class="toc-text">总体算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#算法伪代码"><span class="toc-number">3.</span> <span class="toc-text">算法伪代码</span></a></li></ol></div></div><div class="post-content"><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>PPO算法，该算法基于TRPO算法提出了两种改进的算法，第一种对TRPO的目标函数进行clipping操作，第二种将TRPO的约束条件变为对目标函数的惩罚。该算法和A2C算法有相似之处，比如都是同时使用多个workers进行训练然后综合他们的训练结果。算法的源代码可以在baselines里面找到。</p>
<h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><h3 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h3><h4 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h4><p>最常见的策略梯度算法：<br>$$<br>\hat g=\hat E_t{\triangledown_{\theta}\log\pi_{\theta}(a_t|x_t)\hat A_t}\tag{1}<br>$$<br>其中$\pi_{\theta}$是一个随机策略,$\hat A_t$是在时间t对优势函数的估计。由此函数我们可以得到下面的目标函数：<br>$$<br>L^{PG}(\theta)=\hat E_t[\log\pi_{\theta}(a_t|s_t)\hat A_t]\tag{2}<br>$$<br>该方法有一个很大的缺点，会经常导致很大的策略更新，而很大的策略更新是不靠谱的。</p>
<h4 id="可信域方法"><a href="#可信域方法" class="headerlink" title="可信域方法"></a>可信域方法</h4><p>在TRPO算法论文里面，目标函数是策略更新大小被限制的最大化目标：<br>$$<br>\begin{align}<br>&amp;\text{maximize}_{\theta} \ \hat E_t[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat A_t] \cr<br>&amp;subject \ to \ \hat E_t[KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]]\le \delta<br>\end{align}\tag{3}<br>$$<br>其中，$\theta_{old}$是更新之前的策略参数。该问题可以在对目标函数使用线性近似和对约束使用二次近似后使用共轭梯度算法近似求解。</p>
<h3 id="Clipped-Surrogate-Objective"><a href="#Clipped-Surrogate-Objective" class="headerlink" title="Clipped Surrogate Objective"></a>Clipped Surrogate Objective</h3><p>我们用$r_t(\theta)$代表概率之比，即$r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$,$r(\theta_{old})=1$,那么TRPO的替换目标函数<br>$$<br>L^{CLIP}(\theta)=\hat E_t[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat A_t]=\hat E_t[r_t(\theta)\hat A_t].\tag{4}<br>$$<br>对于该目标函数，最大化它会导致很大的策略更新，所以我们需要对大的策略更新进行惩罚：<br>$$<br>L^{CLIP}(\theta)=\hat E_t[\min(r_t(\theta)\hat A_t,\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat A_t)]\tag{5}<br>$$<br>其中$\epsilon$是一个超参数。该目标函数使用最小值来保证最后的目标函数是一个下界。</p>
<h3 id="使用KL惩罚系数"><a href="#使用KL惩罚系数" class="headerlink" title="使用KL惩罚系数"></a>使用KL惩罚系数</h3><p>对于TRPO的优化问题，里面的限制条件我们可以改为惩罚系数,具体的算法过程如下：</p>
<ol>
<li>使用minibatch SGD算法经过几个epochs优化下面的KL惩罚目标函数<br>$$<br>L^{KLPEN}(\theta)=\hat E_t[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat A_t-\beta KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]]\tag{6}<br>$$</li>
<li>计算$d=\hat E_t[KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]]$</li>
</ol>
<ul>
<li>如果$d&lt; d_{targ}/1.5,\beta\leftarrow\beta/2$,这里说明更新的步伐很小，惩罚变小。</li>
<li>如果$d&gt;d_{targ}\times1.5,\beta\leftarrow\beta\times 2$，这里说明更新的步伐过大，惩罚变大。</li>
</ul>
<p>其中的$d_{targ}$代表的是KL的目标值，是一个超参数。该方法和上面的clip方法相比效果偏差。</p>
<h3 id="总体算法"><a href="#总体算法" class="headerlink" title="总体算法"></a>总体算法</h3><p>PPO算法是基于策略和值函数的，最后的损失函数必然会结合两者:<br>$$<br>L^{CLIP+VF+S}(\theta)=\hat E_t[L^{CLIP}_t(\theta)-c_1L^{VF}_t(\theta)+c_2S[\pi_{\theta}](s_t)]\tag{7}<br>$$<br>其中的$c_1,c_2$是系数，S代表的是信息熵补偿，$L^{VF}_t$代表的是平方误差损失$(V_{\theta}(s_t)-V_t^{targ})^2$.<br>对于策略损失的计算需要估计$\hat A_t$,PPO采用如下方式进行估计：<br>$$<br>\begin{align}<br>&amp;\hat A_t=\delta_t+(\gamma\lambda)\delta_{t+1}+\cdots+\cdots+(\gamma\lambda)^{T-t+1}\delta_{T-1}\cr<br>&amp;其中,\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)\tag{8}<br>\end{align}<br>$$</p>
<h2 id="算法伪代码"><a href="#算法伪代码" class="headerlink" title="算法伪代码"></a>算法伪代码</h2><p><img src="170c8afe027e427eae57b017a25982c5.png" alt="c2aeb4fa31c4ab24d1040866c7374765.png"><br>该算法的流程和baselines的PPO1代码实现是一致的，先采样，然后从里面选取minibatch进行优化，和ACER等算法有细微差别。</p>
</div><div class="tags"><a href="/tags/强化学习/">强化学习</a></div><div class="post-nav"><a class="pre" href="/2019/06/13/Collaborative-Evolutionary-Reinforcement-Learning/">Collaborative Evolutionary Reinforcement Learning</a><a class="next" href="/2019/05/30/ACER算法笔记整理/">Sample Efficient Actor-Critic With Experience Replay</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC80MjMxNi8xODg2Mw"><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"><input type="hidden" name="si" value="https://lightzhan.github.io"><input name="tn" type="hidden" value="bds"><input name="cl" type="hidden" value="3"><input name="ct" type="hidden" value="2097152"><input name="s" type="hidden" value="on"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/基础技能/">基础技能</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知识交流/">知识交流</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程技能/">编程技能</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/强化学习/" style="font-size: 15px;">强化学习</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/计算机网络/" style="font-size: 15px;">计算机网络</a> <a href="/tags/技巧锦集/" style="font-size: 15px;">技巧锦集</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/环境配置/" style="font-size: 15px;">环境配置</a> <a href="/tags/计算机/" style="font-size: 15px;">计算机</a> <a href="/tags/概率统计/" style="font-size: 15px;">概率统计</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/07/05/tmux介绍和使用/">tmux介绍和使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/03/python面向对象编程/">python面向对象编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/ssh隧道代理/">ssh隧道代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/python基础语法扫盲-python2-0/">python基础语法扫盲(python2.0+)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/13/Collaborative-Evolutionary-Reinforcement-Learning/">Collaborative Evolutionary Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/PPO算法/">PPO算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/ACER算法笔记整理/">Sample Efficient Actor-Critic With Experience Replay</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/29/Python变量-引用-对象/">Python变量=引用->对象</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/tensorflow的custom-getter/">tensorflow的custom_getter</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/15/Prioritized-Experience-Replay/">Prioritized Experience Replay</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/lightzhan/" title="My github" target="_blank">My github</a><ul></ul><a href="https://Mr-solution.github.io/Notes" title="Mr-solution" target="_blank">Mr-solution</a><ul></ul><a href="https://kchu.github.io/" title="飞语流年" target="_blank">飞语流年</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Light.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>