<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="人工智能 geek 极客 阅读 电影"><title>Self-Imitation-Learning | Light</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Self-Imitation-Learning</h1><a id="logo" href="/.">Light</a><p class="description">在这座城，等一个人</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/message/"><i class="fa fa-comments"> 留言</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Self-Imitation-Learning</h1><div class="post-meta">May 14, 2019<span> | </span><span class="category"><a href="/categories/学习笔记/">学习笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#论文概述"><span class="toc-number">1.</span> <span class="toc-text">论文概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#算法解读"><span class="toc-number">2.</span> <span class="toc-text">算法解读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#自模仿学习的目标"><span class="toc-number">2.1.</span> <span class="toc-text">自模仿学习的目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#经验回放池"><span class="toc-number">2.2.</span> <span class="toc-text">经验回放池</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#off-policy-AC-loss"><span class="toc-number">2.3.</span> <span class="toc-text">off-policy AC loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Prioritized-Replay"><span class="toc-number">2.4.</span> <span class="toc-text">Prioritized Replay</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A2C-SIL"><span class="toc-number">2.5.</span> <span class="toc-text">A2C+SIL</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理论证明"><span class="toc-number">3.</span> <span class="toc-text">理论证明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验部分"><span class="toc-number">4.</span> <span class="toc-text">实验部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实现细节"><span class="toc-number">4.1.</span> <span class="toc-text">实现细节</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Atari"><span class="toc-number">4.1.1.</span> <span class="toc-text">Atari</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Mujoco"><span class="toc-number">4.1.2.</span> <span class="toc-text">Mujoco</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果"><span class="toc-number">4.2.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Key-Door-Treasure"><span class="toc-number">4.2.1.</span> <span class="toc-text">Key-Door-Treasure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hard-exploration-Atari-Games"><span class="toc-number">4.2.2.</span> <span class="toc-text">Hard exploration Atari Games</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#在Atari游戏上的总体性能"><span class="toc-number">4.2.3.</span> <span class="toc-text">在Atari游戏上的总体性能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MuJoCo-tasks"><span class="toc-number">4.2.4.</span> <span class="toc-text">MuJoCo tasks</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SIL算法的缺陷"><span class="toc-number">5.</span> <span class="toc-text">SIL算法的缺陷</span></a></li></ol></div></div><div class="post-content"><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><p>本论文是ICML2018的一篇论文，提出了Self-Imitation Learning算法。论文的主要思想就是学习智能体过去好的经验以让它重复产生好的决策。</p>
<h2 id="算法解读"><a href="#算法解读" class="headerlink" title="算法解读"></a>算法解读</h2><h3 id="自模仿学习的目标"><a href="#自模仿学习的目标" class="headerlink" title="自模仿学习的目标"></a>自模仿学习的目标</h3><p>模仿智能体自身过去好的经验。</p>
<h3 id="经验回放池"><a href="#经验回放池" class="headerlink" title="经验回放池"></a>经验回放池</h3><p>将过去的经验片段连带累积回报存放在一个回放池里面$D={(s_t,a_t,R_t)}$(存放是所有的经验片段，不是仅仅好的经验片段)。其中$s_t,a_t$是时间t的状态和动作，$R_t=\sum_{k=t}^{\infty}\gamma^{k-t}r_k$是折现因子为$\gamma$的折现奖励之和。</p>
<h3 id="off-policy-AC-loss"><a href="#off-policy-AC-loss" class="headerlink" title="off-policy AC loss"></a>off-policy AC loss</h3><p><img src="2c9912a8a8b2466fb3f110c7f1c95277.png" alt="f000b67098626c90d48a38edc6b59b3e.png"><br>其中$(\cdot)_+=max(\cdot,0)$。$\pi_{\theta},V_{\theta}(s)$分别代表以$\theta$为参数的策略和值函数，$\beta^{sil}\in R^+$是值损失函数的超参数。从$(\cdot)$操作可以看出SIL算法只会模仿它过去那些得到的回报大于期望回报的决策。$L_{value}^{sil}$朝着off-policy回报R更新值估计.</p>
<h3 id="Prioritized-Replay"><a href="#Prioritized-Replay" class="headerlink" title="Prioritized Replay"></a>Prioritized Replay</h3><p>因为只有满足$R&gt;V_{\theta}$的状态动作对会对SIL算法的梯度计算产生贡献，本文提出了使用prioritized experience replay来得到大量的符合这种条件的样本。也就是说这里从replay buffer里面使用clipped优势$(R-V_{\theta}(s))_+$作为优先级采集transitions（采样概率与$(R-V_{\theta}(s))_+$成比例）。因为优势越大，被采集的概率也就越大，这使得得到的transtions对梯度产生贡献的概率也就越大。</p>
<h3 id="A2C-SIL"><a href="#A2C-SIL" class="headerlink" title="A2C+SIL"></a>A2C+SIL</h3><p>本论文选择了A2C+SIL的组合方式来验证算法的可行性。算法伪代码如下：<br><img src="7dca843319024367b1d6ad32840a10bc.png" alt="6993dd44b96709b33a89e8f28f72ebe8.png"><br>A2C的目标函数为：<br><img src="3d071347203b4ff6b51280d9e69f0062.png" alt="98787c7746c6d13b4691ba6892319ce9.png"><br>其中$H_t^{\pi}=-\sum_{a}\pi(a|s_t)\log\pi(a|s_t)$代表简化后的信息熵，$\alpha$是信息熵正则化权重。$V_t^n=\sum_{d=0}^{n-1}\gamma^d r_{t+d}+\gamma^n V_{\theta}(s_{t+n})$是n步自举值。</p>
<h2 id="理论证明"><a href="#理论证明" class="headerlink" title="理论证明"></a>理论证明</h2><p>理论证明部分，就是证明SIL的学习目标函数可以看成是lower-bound-soft-Q-learning在信息熵正则化强化学习框架下的一种实现。<br>详细的证明过程可以参考原论文。最后的证明结论就是SIL可以被看成是朝着最优策略和最优值更新策略$\pi_{\theta}$和值$V_{\theta}$.</p>
<h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>本文的代码实现主要有两个：A2C+SIL和PPO+SIL.</p>
<h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><h4 id="Atari"><a href="#Atari" class="headerlink" title="Atari"></a>Atari</h4><p>对于Atari实验，使用了3层卷积神经网络，并且将最后的4帧用于输入。在没有AC更新过后进行4次SIL学习(M=4)。这里将游戏的结束作为回合片段的结束，而不是将失去一条生命作为回合片段的结束。</p>
<h4 id="Mujoco"><a href="#Mujoco" class="headerlink" title="Mujoco"></a>Mujoco</h4><p>对于Mujoco实验，使用了具有两个隐藏层具有64个神经元的MLP，每次迭代进行10次的自模仿学习。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="Key-Door-Treasure"><a href="#Key-Door-Treasure" class="headerlink" title="Key-Door-Treasure"></a>Key-Door-Treasure</h4><p>该实验用于探索SIL如何有利于探索以及它是否和Count-Based探索方法相兼容。<br><img src="1eb7bb76d8aa4e6a932381efbd71239c.png" alt="6483344663677b4d70fc36cfbcdb1b87.png"><br>实验结果说明两者是相兼容的。</p>
<h4 id="Hard-exploration-Atari-Games"><a href="#Hard-exploration-Atari-Games" class="headerlink" title="Hard exploration Atari Games"></a>Hard exploration Atari Games</h4><p>下面是在6个Hard Exploration Atati Games的测试结果：<br><img src="038bc383b6414a68b5d561f3989c52e1.png" alt="b4fb9d68247d6d8d703f4b474e1fb47c.png"><br>下面是在这几个Hard exploration Atari Games上SIL与其它Count-based exploration方法的比较结果：<br><img src="9ae506b9cc6a4283ad0a1c2e6ebf845f.png" alt="7d633a7dba20e4c4140f3981b7555e9d.png"><br>这以看到在前6个游戏上，A2C+SIL的结果优于其它算法，但是对于最后一个游戏VENTURE，A2C+SIL没有得到任何的奖励，导致没有好的experience用于exploit，这点值得我们深思！</p>
<h4 id="在Atari游戏上的总体性能"><a href="#在Atari游戏上的总体性能" class="headerlink" title="在Atari游戏上的总体性能"></a>在Atari游戏上的总体性能</h4><p>在49个Atari游戏上经过50M steps(200M frames)训练后的结果：<br><img src="1b3368b26b384225b88e883ba9ebd268.png" alt="13be54bf47c79e3f1231ba1b6cfb34a9.png"><br>下面是A2C和A2C+SIL在49个游戏上的相对性能比较：<br><img src="70172e280a044333b17de901dd7b191d.png" alt="e288800bc674afa828e41881ff15dc9f.png"><br>可以看出在35个游戏上A2C+SIL的性能超过了A2C。</p>
<h4 id="MuJoCo-tasks"><a href="#MuJoCo-tasks" class="headerlink" title="MuJoCo tasks"></a>MuJoCo tasks</h4><p>这里探索SIL算法是否有利于连续性控制任务以及是否能被用于其他类型的策略优化算法，比如PPO。作者实现了PPO+SIL算法。<br><img src="75c1e19a430c42d9a8c08ecd0bc5f3f3.png" alt="a1945d010e3c72952cb7b3577e38058c.png"><br>下面一行是延迟奖励的测试结果，显示当奖励被延迟（比如每20步给予奖励而不是每步都给予奖励）的时候SIL的优势更加明显。</p>
<h2 id="SIL算法的缺陷"><a href="#SIL算法的缺陷" class="headerlink" title="SIL算法的缺陷"></a>SIL算法的缺陷</h2><ul>
<li>在Atari实验中，SIL算法经常在开始阶段学习更快，但是有时候在一些游戏中会陷入局部最优解。这说明在学习的早期阶段过多地exploition会影响性能。论文作者发现减少每次迭代SIL更新的次数或者在后面的学习阶段中为SIL目标函数使用一个小的权重能解决该问题甚至提升在这些游戏上的性能。</li>
<li>通过Mujoco实验我们可以发现在连续控制性任务上，SIL算法的表现并不是特别好（与原算法相比并没有提升很明显）。</li>
</ul>
</div><div class="tags"><a href="/tags/强化学习/">强化学习</a><a href="/tags/论文阅读/">论文阅读</a></div><div class="post-nav"><a class="pre" href="/2019/05/15/Prioritized-Experience-Replay/">Prioritized Experience Replay</a><a class="next" href="/2019/05/13/动态规划/">动态规划</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC80MjMxNi8xODg2Mw"><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"><input type="hidden" name="si" value="https://lightzhan.github.io"><input name="tn" type="hidden" value="bds"><input name="cl" type="hidden" value="3"><input name="ct" type="hidden" value="2097152"><input name="s" type="hidden" value="on"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/基础技能/">基础技能</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知识交流/">知识交流</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程技能/">编程技能</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/强化学习/" style="font-size: 15px;">强化学习</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/计算机网络/" style="font-size: 15px;">计算机网络</a> <a href="/tags/技巧锦集/" style="font-size: 15px;">技巧锦集</a> <a href="/tags/计算机/" style="font-size: 15px;">计算机</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/环境配置/" style="font-size: 15px;">环境配置</a> <a href="/tags/概率统计/" style="font-size: 15px;">概率统计</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/07/05/tmux介绍和使用/">tmux介绍和使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/03/python面向对象编程/">python面向对象编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/ssh隧道代理/">ssh隧道代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/python基础语法扫盲-python2-0/">python基础语法扫盲(python2.0+)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/13/Collaborative-Evolutionary-Reinforcement-Learning/">Collaborative Evolutionary Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/PPO算法/">PPO算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/ACER算法笔记整理/">Sample Efficient Actor-Critic With Experience Replay</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/29/Python变量-引用-对象/">Python变量=引用->对象</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/tensorflow的custom-getter/">tensorflow的custom_getter</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/15/Prioritized-Experience-Replay/">Prioritized Experience Replay</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/lightzhan/" title="My github" target="_blank">My github</a><ul></ul><a href="https://Mr-solution.github.io/Notes" title="Mr-solution" target="_blank">Mr-solution</a><ul></ul><a href="https://kchu.github.io/" title="飞语流年" target="_blank">飞语流年</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Light.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>