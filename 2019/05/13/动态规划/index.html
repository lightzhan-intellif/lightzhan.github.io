<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="人工智能 geek 极客 阅读 电影"><title>动态规划 | Light</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">动态规划</h1><a id="logo" href="/.">Light</a><p class="description">在这座城，等一个人</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/message/"><i class="fa fa-comments"> 留言</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">动态规划</h1><div class="post-meta">May 13, 2019<span> | </span><span class="category"><a href="/categories/学习笔记/">学习笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是动态规划DP？"><span class="toc-number">1.</span> <span class="toc-text">什么是动态规划DP？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#强化学习中的DP"><span class="toc-number">2.</span> <span class="toc-text">强化学习中的DP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#策略评估"><span class="toc-number">3.</span> <span class="toc-text">策略评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#策略提升"><span class="toc-number">4.</span> <span class="toc-text">策略提升</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#策略迭代"><span class="toc-number">5.</span> <span class="toc-text">策略迭代</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#值迭代"><span class="toc-number">6.</span> <span class="toc-text">值迭代</span></a></li></ol></div></div><div class="post-content"><h2 id="什么是动态规划DP？"><a href="#什么是动态规划DP？" class="headerlink" title="什么是动态规划DP？"></a>什么是动态规划DP？</h2><p>动态规划指的是根据环境的完美模型，依据马尔可夫决策过程计算最优化策略的算法集合。也就是说动态规划算法是基于模型（model-based）的一种方法。</p>
<h2 id="强化学习中的DP"><a href="#强化学习中的DP" class="headerlink" title="强化学习中的DP"></a>强化学习中的DP</h2><p>常规强化学习中的DP算法的关键思想是使用值函数来搜索好的策略。</p>
<h2 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h2><p>所谓的策略评估，就是对给定的任意策略计算其状态值函数$v_{\pi}$.策略评估也叫预测问题。根据$v_{\pi}$的公式我们有：<br>$$<br>\begin{align}<br>v_{\pi}(s)&amp;=E_{\pi}[G_t|S_t=s] \cr<br>&amp;=E_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s] \cr<br>&amp;=\sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a)[r+\gamma v_{\pi}(s’)]<br>\end{align}<br>$$<br>其中$\pi(a|s)$是在状态s采取动作a的概率。期望的下标$\pi$指得是期望的计算是根据策略$\pi$。$v_{\pi}$存在和唯一的条件是只要$\gamma&lt;1$或者所有的状态依据策略$\pi$都有终止态。<br>如果环境的动态转移特性完全可知，我们就能得到<strong>迭代策略评估</strong>方法:<br>$$<br>\begin{align}<br>v_{k+1}(s)&amp;\doteq E_{\pi}[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s] \cr<br>&amp;=\sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a)[r+\gamma v_k(s’)]<br>\end{align}<br>$$<br>等式里面的$v_{k+1}(s)$和$v_k(s)$代表是状态s在第k+1次迭代和第k次迭代的评估值。从等式可以看出v(s)的第k+1次评估值的计算使用的是下一个状态s’的第k次评估值，也就是说用旧的值更新新的值。<br>对于该迭代策略评估方法的计算机实现有两种。第一种是使用两个数组，一个保存新的迭代计算值另一个保存旧的值；第二种方法是使用一个数组，新计算出来的评估值直接替代旧的的评估值(in-place替代)，也就是上式右边计算使用的是新的迭代值$v_{k+1}(s’)$,这种方法不仅会收敛到$v_{\pi}$,而且收敛的速度会更快，因为计算产生的新的评估值会被立即使用。所以一般说到的DP算法都是使用的in-place替代。下面是in-place替代计算的伪代码：<br><img src="abe4c33d7d2b4ef7b5580bfc04d5dc37.png" alt="6a764761389c91f99619b43caee8a9f4.png"><br>注意该算法的结束方式是策略评估值的更新小于$\theta$。</p>
<h2 id="策略提升"><a href="#策略提升" class="headerlink" title="策略提升"></a>策略提升</h2><p>根据原始策略的值函数进行贪婪动作选择，以制定一个新的策略来提升旧的策略的过程叫做策略提升.<br>这种行为方式的价值就是：<br>$$<br>\begin{align}<br>q_{\pi}(s,a)&amp;\doteq E[R_{t+1}+\gamma  v_{\pi}(S_{t+1})|S_t=s,A_t=a] \cr<br>&amp;=\sum_{s’,r}p(s’,r|s,a)[r+\gamma v_{\pi}(s’)]<br>\end{align}<br>$$<br>判断在状态s选择动作a是否是更好的策略的关键判断标准是$q_{\pi}(s,a)$是否大于$v_{\pi}(s)$,如果大于，那么该策略就是比原策略更优的，这就是<strong>策略提升理论</strong>。<br>也就是说，对于任何的状态$s\in S$,$\pi$和$\pi’$是任意的两个确定性策略(我们这里暂时只讨论确定性策略)，如果有<br>$$<br>q_{\pi}(s,\pi’(s))\geq v_{\pi}(s)\tag{1}<br>$$<br>那么策略$\pi’$肯定是比策略$\pi$更好或者相持平的策略，也就是说对所有的$s\in S$:<br>$$<br>v_{\pi’}(s)\geq v_{\pi}(s)\tag{2}<br>$$<br>下面我们证明从1式推导2式。<br>$$<br>\begin{align}<br>v_{\pi}&amp;\leq q_{\pi}(s,\pi’(s))\cr<br>&amp;=E[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s,A_t=\pi’(s)]\cr<br>&amp;=E_{\pi’}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s]\cr<br>&amp;\leq E_{\pi’}[R_{t+1}+\gamma q_{\pi}(S_{t+1},\pi’(S_{t+1}))|S_t=s]\cr<br>&amp;\vdots\cr<br>&amp;\leq E_{\pi’}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\cdots|S_t=s]\cr<br>&amp;=v_{\pi’}(s)<br>\end{align}<br>$$<br>什么时候策略提升找到最优策略呢？我们假设一个新的策略$\pi’$和旧的策略$\pi$一样好而没有更好，对于所有$s\in S$:<br>$$<br>\begin{align}<br>v_{\pi’}(s)&amp;=\max_a E[R_{t+1}+\gamma v_{\pi’}(S_{t+1})|S_t=s,A_t=a]\cr<br>&amp;=\max_a \sum_{s’,r}p(s’,r|s,a)[r+\gamma v_{\pi’}(s’)]<br>\end{align}<br>$$<br>可以发现该等式和贝尔曼最优方程一样，因此$v_{\pi’}$一定就是$v_*$,$\pi$和$\pi’$一定就是最优策略。也就是说策略提升一定会提供一个更好的策略，除非原始策略已经是最优策略。</p>
<h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p><img src="9da4633a11384986b63722b27060e852.png" alt="5dcc6413d7f35bc52de5129b2523b55a.png"><br>上图中，$\stackrel{E}{\longrightarrow}$代表策略评估，$\stackrel{I}{\longrightarrow}$代表策略提升。这种策略评估和策略提升的不断迭代直到找到最优策略的方法叫做策略迭代。下面是策略迭代的伪代码：<br><img src="0ee6538e027341eabea0aa5d47d65266.png" alt="9c535a8724c8e9828fe288353ce00725.png"><br>可以看到先策略评估然后策略迭代，不断循环直到找到最优策略。</p>
<h2 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h2><p>策略迭代存在一个缺陷：每次迭代都要进行策略评估，而策略评估需要大量的迭代计算。相对应的，我们将策略评估仅仅更新状态值一次的算法称为值迭代。这里我们可以给出策略评估和策略提升相结合的更新操作,对于所有的$s\in S$：<br>$$<br>\begin{align}<br>v_{k+1}(s)&amp;\doteq \max_a E[R_{t+1}+\gamma v_{k}(S_{t+1})|S_t=s,A_t=a] \cr<br>&amp;=\max_a \sum_{s’,r}p(s’,r|s,a)[r+\gamma v_k(s’)]<br>\end{align}<br>$$<br>该等式还可以参考贝尔曼最优等式进行理解。值迭代和策略迭代的不同点在于值迭代对状态s的所有动作取价值最大的那个，而且只用进行一次即可，而策略迭代需要先进行策略评估，策略评估就需要迭代很多次，然后再进行策略提升。下面给出值迭代的伪代码：<br><img src="d9d6db3936974357a28783e6b4a5d061.png" alt="e6ab7a64fb5885ce9d49e0e1ae69dcc0.png"><br>值迭代的终止条件是在一次更新中更新的值小于某个阈值$\theta$。值迭代的收敛速度一般要比策略迭代快，因为它没有策略迭代的策略评估迭代那么多次，而且将策略评估和策略迭代结合在了一次操作中（使用max操作），大大减少了运算迭代次数。</p>
</div><div class="tags"><a href="/tags/强化学习/">强化学习</a></div><div class="post-nav"><a class="pre" href="/2019/05/14/Self-Imitation-Learning/">Self-Imitation-Learning</a><a class="next" href="/2019/05/09/有限马尔科夫决策过程/">有限马尔科夫决策过程</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC80MjMxNi8xODg2Mw"><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"><input type="hidden" name="si" value="https://lightzhan.github.io"><input name="tn" type="hidden" value="bds"><input name="cl" type="hidden" value="3"><input name="ct" type="hidden" value="2097152"><input name="s" type="hidden" value="on"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/基础技能/">基础技能</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知识交流/">知识交流</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程技能/">编程技能</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/强化学习/" style="font-size: 15px;">强化学习</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/计算机网络/" style="font-size: 15px;">计算机网络</a> <a href="/tags/技巧锦集/" style="font-size: 15px;">技巧锦集</a> <a href="/tags/计算机/" style="font-size: 15px;">计算机</a> <a href="/tags/环境配置/" style="font-size: 15px;">环境配置</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/概率统计/" style="font-size: 15px;">概率统计</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/07/03/python面向对象编程/">python面向对象编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/ssh隧道代理/">ssh隧道代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/python基础语法扫盲-python2-0/">python基础语法扫盲(python2.0+)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/13/Collaborative-Evolutionary-Reinforcement-Learning/">Collaborative Evolutionary Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/PPO算法/">PPO算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/ACER算法笔记整理/">Sample Efficient Actor-Critic With Experience Replay</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/29/Python变量-引用-对象/">Python变量=引用->对象</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/tensorflow的custom-getter/">tensorflow的custom_getter</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/15/Prioritized-Experience-Replay/">Prioritized Experience Replay</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/Self-Imitation-Learning/">Self-Imitation-Learning</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/lightzhan/" title="My github" target="_blank">My github</a><ul></ul><a href="https://Mr-solution.github.io/Notes" title="Mr-solution" target="_blank">Mr-solution</a><ul></ul><a href="https://kchu.github.io/" title="飞语流年" target="_blank">飞语流年</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Light.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>