<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="人工智能 geek 极客 阅读 电影"><title>有限马尔科夫决策过程 | Light</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">有限马尔科夫决策过程</h1><a id="logo" href="/.">Light</a><p class="description">在这座城，等一个人</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/message/"><i class="fa fa-comments"> 留言</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">有限马尔科夫决策过程</h1><div class="post-meta">May 9, 2019<span> | </span><span class="category"><a href="/categories/学习笔记/">学习笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#状态的马尔可夫性"><span class="toc-number">1.</span> <span class="toc-text">状态的马尔可夫性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是马尔可夫决策过程"><span class="toc-number">2.</span> <span class="toc-text">什么是马尔可夫决策过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#强化学习的目标"><span class="toc-number">3.</span> <span class="toc-text">强化学习的目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Episodic-and-Continuing-Tasks"><span class="toc-number">4.</span> <span class="toc-text">Episodic and Continuing Tasks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#最优策略和最优值函数"><span class="toc-number">5.</span> <span class="toc-text">最优策略和最优值函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#状态值函数"><span class="toc-number">5.1.</span> <span class="toc-text">状态值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#动作值函数"><span class="toc-number">5.2.</span> <span class="toc-text">动作值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优策略和最优值函数-1"><span class="toc-number">5.3.</span> <span class="toc-text">最优策略和最优值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝尔曼最优方程"><span class="toc-number">5.4.</span> <span class="toc-text">贝尔曼最优方程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用最优值函数确定最优策略"><span class="toc-number">6.</span> <span class="toc-text">使用最优值函数确定最优策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#使用最优状态值函数确定最优策略"><span class="toc-number">6.1.</span> <span class="toc-text">使用最优状态值函数确定最优策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用最优动作值函数确定最优策略"><span class="toc-number">6.2.</span> <span class="toc-text">使用最优动作值函数确定最优策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#近似最优化解"><span class="toc-number">7.</span> <span class="toc-text">近似最优化解</span></a></li></ol></div></div><div class="post-content"><h2 id="状态的马尔可夫性"><a href="#状态的马尔可夫性" class="headerlink" title="状态的马尔可夫性"></a>状态的马尔可夫性</h2><p>状态包含过去智能体（Agent）和环境相互作用的所有会对将来产生影响的信息。注意马尔可夫性是针对状态来说的。</p>
<h2 id="什么是马尔可夫决策过程"><a href="#什么是马尔可夫决策过程" class="headerlink" title="什么是马尔可夫决策过程"></a>什么是马尔可夫决策过程</h2><p>对于一个轨迹序列：<br>$$<br>S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,…<br>$$<br>每一个可能状态$S_t$和奖励值$R_t$只取决于前一个状态$S_{t-1}$和动作$A_{t-1}$，与更早的状态和动作无关。</p>
<h2 id="强化学习的目标"><a href="#强化学习的目标" class="headerlink" title="强化学习的目标"></a>强化学习的目标</h2><p>强化学习的目标是最大化从长远来看的累积期望回报。<br>其中未来的期望回报需要使用折现率$\gamma$进行折现，最后得到的最大化的期望折现回报值就是：<br>$$<br>G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}<br>$$<br>其中折现率(discount rate)$\gamma$满足$0\le\gamma\le 1$.当$\gamma=0$的时候，$G_t=R_{t+1}$，也就是说这时候最大化的期望回报就是即是奖励，不考虑将来的回报，这时候的Agent就是短视的；当$\gamma$趋近于1的时候，期望回报更看重未来的奖励，这时候Agent就更加具有远见。我们将$G_t$写成贝尔曼方程的形式：<br>$$<br>\begin{align}<br>G_t&amp;\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3R_{t+4}+\cdots\cr<br>&amp;=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\gamma^2R_{t+4}+\cdots)\cr<br>&amp;=R_{t+1}+\gamma G_{t+1}<br>\end{align}<br>$$</p>
<h2 id="Episodic-and-Continuing-Tasks"><a href="#Episodic-and-Continuing-Tasks" class="headerlink" title="Episodic and Continuing Tasks"></a>Episodic and Continuing Tasks</h2><ul>
<li>Episodic tasks: 有明确的结束点；</li>
<li>Continuing tasks: 没有明确的结束点，任务会一只进行下去，其折现回报值可能是无穷大的。</li>
</ul>
<h2 id="最优策略和最优值函数"><a href="#最优策略和最优值函数" class="headerlink" title="最优策略和最优值函数"></a>最优策略和最优值函数</h2><p>我们先看两个重要的概念：状态值函数和动作值函数。然后我们再基于这两个概念引入最优策略和最优值函数。</p>
<h3 id="状态值函数"><a href="#状态值函数" class="headerlink" title="状态值函数"></a>状态值函数</h3><p>在策略$\pi$下，状态s的值函数使用$v_{\pi}(s)$来表示，其代表的意义是从状态s开始遵循策略$\pi$的期望回报。对于MDPs，我们可以得到:<br>$$<br>v_{\pi}(s)\doteq E_{\pi}[G_t|S_t=s]=E_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}|S_t=s],for \ all \ s \in S,<br>$$<br>$v_{\pi}$被称为策略$\pi$的状态值函数。注意终止状态的$v_{\pi}$值总是0.<br>下面我们推到$v_{\pi}(s)$的贝尔曼方程(表达状态s和它的后继状态s’之间的关系)：<br>$$<br>\begin{align}<br>v_{\pi}(s)&amp;\doteq E_{\pi}[G_t|S_s=s] \cr<br>&amp;=E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s] \cr<br>&amp;=\sum_a \pi(a|s) \sum_{s’}\sum_{r}p(s’,r|s,a)[r+\gamma E_{\pi}[G_{t+1}|S_{t+1}=s’]] \cr<br>&amp;=\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)[r+\gamma v_{\pi}(s’)],for \ all \ s\in S,<br>\end{align}<br>$$</p>
<h3 id="动作值函数"><a href="#动作值函数" class="headerlink" title="动作值函数"></a>动作值函数</h3><p>根据策略$\pi$，在状态s下采取动作a的期望回报值就是策略$\pi$的动作值函数，使用$q_{\pi}(s,a)$来表示：<br>$$<br>q_{\pi}(s,a)\doteq E_{\pi}[G_t|S_t=s,A_t=a]=E_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}|S_t=s,A_t=a],<br>$$</p>
<h3 id="最优策略和最优值函数-1"><a href="#最优策略和最优值函数-1" class="headerlink" title="最优策略和最优值函数"></a>最优策略和最优值函数</h3><p>所谓的最优策略，就是该策略比其它策略更好或者相当。最优策略可能不止一个，我们使用$\pi_\ast$代表这些最优策略。这些最优策略共享同一个状态状态值函数——最优状态值函数$v_\ast$:<br>$$<br>v_\ast(s)\doteq \max_{\pi}v_{\pi}(s)<br>$$<br>除此而外，最优策略也共享相同的最优动作值函数$q_\ast$:<br>$$<br>q_\ast(s,a)\doteq \max_{\pi}q_{\pi}(s,a), for \ all \ s\in S \ and \ a\in A(s)<br>$$<br>我们可以根据$v_\ast$来表示$q_\ast$:<br>$$<br>q_\ast(s,a)=E[R_{t+1}+\gamma v_\ast(S_{t+1})|S_t=s,A_t=a]<br>$$<br>该等式的含义是在状态s采取动作a的期望回报，之后的动作选取遵循一个最优策略(也就是$\gamma v_\ast(S_{t+1})$项的意义)</p>
<h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>这里我们推导最优状态值函数和最优动作值函数的贝尔曼方程：<br>$$<br>\begin{align}<br>v_\ast(s)&amp;=\max_{a\in A(s)}q_{\pi_\ast(s,a)} \cr<br>&amp;=\max_a E_{\pi_\ast}[G_t|S_t=s,A_t=a] \cr<br>&amp;=\max_s E_{\pi_\ast}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a] \cr<br>&amp;=\max_a E[R_{t+1}+\gamma v_\ast(S_{t+1})|S_t=s,A_t=a] \cr<br>&amp;=\max_a \sum_{s’,r}p(s’,r|s,a)[r+\gamma v_\ast(s’)]<br>\end{align}<br>$$<br>上面最后两行就是$v_\ast(s)$的最优贝尔曼方程。下面推导$q_\ast$的最优贝尔曼方程：<br>$$<br>\begin{align}<br>q_\ast(s,a)&amp;=E[R_{t+1}+\gamma \max_{a’}q_\ast(S_{t+1},a’)|S_t=s,A_t=a] \cr<br>&amp;=\sum_{s’,r}p(s’,r|s,a)[r+\gamma \max_{a’}q_\ast(s’,a’)]<br>\end{align}<br>$$</p>
<h2 id="使用最优值函数确定最优策略"><a href="#使用最优值函数确定最优策略" class="headerlink" title="使用最优值函数确定最优策略"></a>使用最优值函数确定最优策略</h2><h3 id="使用最优状态值函数确定最优策略"><a href="#使用最优状态值函数确定最优策略" class="headerlink" title="使用最优状态值函数确定最优策略"></a>使用最优状态值函数确定最优策略</h3><p>使用最优状态值函数确定在状态s时采取的最优策略动作，就是在s进行一步超前搜索，得到动作a，使得执行该动作a后得到的下一个状态$s_{t+1}$是所有状态中状态值最大的。该方法的使用需要满足下面三个条件：</p>
<ol>
<li>我们需要精确地知道环境的动态转移特性，也就是我们要知道环境的状态转移矩阵；</li>
<li>因为该方法需要一步朝前搜索，即要搜索下一个可能的所有状态，如果状态的数量非常巨大，那么搜索必然是一个非常麻烦的问题，需要我们具有足够的计算资源；</li>
<li>状态满足马尔可夫性质；</li>
</ol>
<p>因为很多时候计算资源很难满足，我们就会采用近似解来逼近真实解（比如采用启发法），这样来减少计算的开销同时得到一个比较满意的解。</p>
<h3 id="使用最优动作值函数确定最优策略"><a href="#使用最优动作值函数确定最优策略" class="headerlink" title="使用最优动作值函数确定最优策略"></a>使用最优动作值函数确定最优策略</h3><p>使用最优动作值函数$q_\ast(s,a)$确定最优策略不需要像使用最优状态值函数那样使用一步朝前搜索，只需要找到$\max_a q_\ast(s,a)$的动作a即可。因为不需要进行一步朝前搜索，所以使用该函数是不需要知道该状态的下一个可能状态以及可能状态的值的，也就是不需要知道环境的动态特性(dynamics)。</p>
<h2 id="近似最优化解"><a href="#近似最优化解" class="headerlink" title="近似最优化解"></a>近似最优化解</h2><p>上面提到的最优化值函数和最优化策略在理论上可以得到很好的效果，但是现实中面对这两个问题：1.算力很难满足；2. 计算机内存有限。因为很多时候我们面对的问题的状态数量是非常巨大的，无论是最优策略的计算还是值的保存都是一件非常困难的事情，这时候就需要我们使用近似的方法来解决问题。比如我们可以更加关注经常遇到的状态而减少对不怎么出现的状态的关注。</p>
</div><div class="tags"><a href="/tags/强化学习/">强化学习</a></div><div class="post-nav"><a class="pre" href="/2019/05/13/动态规划/">动态规划</a><a class="next" href="/2019/05/08/强化学习概述/">强化学习概述</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC80MjMxNi8xODg2Mw"><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"><input type="hidden" name="si" value="https://lightzhan.github.io"><input name="tn" type="hidden" value="bds"><input name="cl" type="hidden" value="3"><input name="ct" type="hidden" value="2097152"><input name="s" type="hidden" value="on"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/基础技能/">基础技能</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知识交流/">知识交流</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程技能/">编程技能</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/强化学习/" style="font-size: 15px;">强化学习</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/计算机/" style="font-size: 15px;">计算机</a> <a href="/tags/概率统计/" style="font-size: 15px;">概率统计</a> <a href="/tags/环境配置/" style="font-size: 15px;">环境配置</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/13/Collaborative-Evolutionary-Reinforcement-Learning/">Collaborative Evolutionary Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/PPO算法/">PPO算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/ACER算法笔记整理/">Sample Efficient Actor-Critic With Experience Replay</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/29/Python变量-引用-对象/">Python变量=引用->对象</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/tensorflow的custom-getter/">tensorflow的custom_getter</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/15/Prioritized-Experience-Replay/">Prioritized Experience Replay</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/Self-Imitation-Learning/">Self-Imitation-Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/13/动态规划/">动态规划</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/09/有限马尔科夫决策过程/">有限马尔科夫决策过程</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/08/强化学习概述/">强化学习概述</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/lightzhan/" title="My github" target="_blank">My github</a><ul></ul><a href="https://Mr-solution.github.io/Notes" title="Mr-solution" target="_blank">Mr-solution</a><ul></ul><a href="https://kchu.github.io/" title="飞语流年" target="_blank">飞语流年</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Light.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>